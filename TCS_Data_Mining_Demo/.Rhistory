tweets = searchTwitter("#abortion", n=1000)
tweets = searchTwitter("#Juno", n=1000)
intsall.packages("sentiment")
install.packages("sentiment")
install.packages("wordcloud")
install.packages("twitteR")
install.packages("ROAuth")
install.packages("stringr")
install.packages("ggplot2")
install.packages("tm")
install.packages("snowballC")
install.packages("SnowballC")
myCorpus <- Corpus(vectorScource(myTweets$Tweets))
install.packages("tm")
myCorpus <- Corpus(vectorScource(myTweets$Tweets))
require(tm)
require(twitteR)
require(ROAuth)
require(plyr)
require(dplyr)
require(stringr)
require(ggplo)
require(ggplot2)
require(wordcloud)
require(SnowballC)
require(RColorBrewer)
library(ggplot2)
print("complete")
print(myCorpus.length)
lenth(myCorpus)
length(myCorpus)
length(myTweets)
length(myTweets)
myTweets <- read.csv("data\2016_07_14_08_32_23.csv")
myTweets <- read.csv("C:\\Users\\priyabhatnagar\\TCS_Data_Mining_Demo\\data")
getwd()
clear()
clearPushBack()
#set working directory
setwd("C:\\Users\\priyabhatnagar\\TCS_Data_Mining_Demo")
#install and load packages
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(wordcloud)
install.packages("dplyr")
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
#use sample data
myTweets <- read.csv("C:\\Users\\priyabhatnagar\\TCS_Data_Mining_Demo\\data")
#create Corpus
myCorpus <- Corpus(vectorScource(myTweets$Tweets))
myCorpus <- Corpus(vectorSource(myTweets$Tweets))
dir()
myTweets <- read.csv("data\\tweetCSV.csv")
#create Corpus
myCorpus <- Corpus(vectorSource(myTweets$Tweets))
myCorpus <- Corpus(VectorSource(myTweets$Tweets))
myTweets <- read.csv("data\\tweetCSV.csv")
length(myTweets)
myCorpus <- tm_map(myCorpus, removeWords, stopwords(kind = "en"))
stopwords()
stemDocument("dumpling")
stemDocument("liking")
pal <- brewer.pal(10, "Dark2")
pal <- brewer.pal(8, "Dark2")
wordcloud(myCorpus, min.freq = 4, max_words = Inf, width = 1000, height = 1000, random.order = FALSE, color-pal)
wordcloud(myCorpus, min.freq = 4, max_words = Inf, width = 1000, height = 1000, random.order = FALSE, color=pal)
wordcloud(myCorpus, min.freq = 4, max_words = Inf, random.order = FALSE, color=pal)
wordcloud(myCorpus, min.freq = 4, max.words = Inf, random.order = FALSE, color=pal)
myCorpus <- tm_map(myCorpus, removeWords, stopwords(kind = "en"))
#stemming: shortening the words
myCorpus <- tm_map(myCorpus, stemDocument)
#cloud
pal <- brewer.pal(8, "Dark2")
wordcloud(myCorpus, min.freq = 4, max.words = Inf, width = 1000, height = 1000, random.order = FALSE, color=pal)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
erase.screen()
erase.screen(TRUE)
TRUE
myCorpus <- tm_map(myCorpus, removeWords, stopwords(kind = "en"))
#stemming: shortening the words
myCorpus <- tm_map(myCorpus, stemDocument)
#cloud
pal <- brewer.pal(8, "Dark2")
wordcloud(myCorpus, min.freq = 4, max.words = Inf, width = 1000, height = 1000, random.order = FALSE, color=pal)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
wordcloud(myCorpus, min.freq = 4, max.words = 100, width = 1000, height = 1000, random.order = FALSE, color=pal)
myCorpus <- tm_map(myCorpus, removeWords, stopwords(kind = "en"))
#stemming: shortening the words
myCorpus <- tm_map(myCorpus, stemDocument)
#set working directory
setwd("C:\\Users\\priyabhatnagar\\TCS_Data_Mining_Demo")
#install and load packages
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(tm)
library(RColorBrewer)
myTweets <- read.csv("data\\tweetCSV.csv")
print(myTweets)
